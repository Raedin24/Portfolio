---
title: "Fine-Tuned Channel–Spatial Attention Transformer for Facial Expression Recognition"
summary: "Lightweight FER model combining channel and spatial attention to improve accuracy under limited data."
publishedAt: "2024-08-10"
images: 
  - "/images/projects/rf-emotion-recognition-rf-830x495-1.jpeg"
team:
  - name: "Kofi Corletey"
    avatar: "/images/avatar.jpg"
    linkedIn: "https://www.linkedin.com/in/matey-corletey/"
  - name: "Ebenezer Acquah"
    avatar: "/images/avatar-02.jpg"
    linkedIn: "https://www.linkedin.com/in/eben-success/"

---
# Fine-Tuned Channel–Spatial Attention Transformer (FT-CSAT)

This project implements the **Fine-Tuned Channel–Spatial Attention Transformer (FT-CSAT)** — a compact FER model that fuses a transformer backbone with channel and spatial attention, and uses a parameter-efficient fine-tuning method (Scaling & Shifting Features — SSF) to adapt pretrained weights for facial expression recognition under real-world conditions.

## Overview

FT-CSAT combines a **CSWin-T** transformer backbone with a lightweight **Channel–Spatial Attention (CBAM)** module and **SSF** fine-tuning layers. The goals were:

- Improve robustness to occlusion, pose variance, and noisy images.  
- Keep the number of trainable parameters low via SSF so fine-tuning is feasible on limited hardware.  
- Provide interpretable attention maps that highlight salient facial regions.

---

## Architecture

FT-CSAT augments the transformer backbone with three core components: Channel Attention (CA), Spatial Attention (SA), and the Scaling & Shifting Fine-Tuning (SSF) layer.

### 1. Channel Attention (CA)

Given an intermediate feature map $ F \in \mathbb{R}^{C \times H \times W} $, channel attention models inter-channel dependencies:

$$
M_c(F) = \sigma\!\big(W_2 \,\delta(W_1 \,\mathrm{GAP}(F))\big),
\qquad
F_c = M_c(F) \odot F
$$

Where:
- $ \mathrm{GAP}(\cdot) $ = global average pooling → $ \mathbb{R}^C $  
- $ W_1 \in \mathbb{R}^{\frac{C}{r} \times C},\; W_2 \in \mathbb{R}^{C \times \frac{C}{r}} $ are learned weights  
- $ \delta $= ReLU, $ \sigma $= sigmoid, and $ \odot $ denotes channel-wise scaling

### 2. Spatial Attention (SA)

A spatial mask highlights discriminative facial regions:

$$
M_s(F_c) = \sigma\!\big(f_{7\times7}([\mathrm{AvgPool}(F_c);\ \mathrm{MaxPool}(F_c)])\big),
\qquad
F_{cs} = M_s(F_c) \odot F_c
$$

Here $ f_{7\times7} $ is a 7 × 7 convolution and $[\,;\,]$ denotes concatenation across the channel axis.

### 3. Combined Channel–Spatial Attention

Applied sequentially:

$$
F' = M_s(F) \odot \big(M_c(F) \odot F\big)
$$

This first selects informative channels (the “what”) and then focuses spatially (the “where”), producing richer, emotion-aware feature representations.

### 4. Scaling & Shifting Fine-Tuning (SSF)

SSF introduces small learnable scaling and shifting tensors to adapt frozen backbone features with minimal extra parameters:

$$
\hat{F} = \gamma \odot F' + \beta
$$

where $ \gamma $ and $ \beta $ are learnable tensors (shaped per channel) — this enables parameter-efficient fine-tuning (only SSF params and the classification head are updated while most backbone weights remain frozen).

---

## Pipeline

1. Input face image → CSWin-T backbone (pretrained)  
2. Intermediate features → CBAM block (CA → SA)  
3. SSF layers modulate the CBAM output for domain adaptation  
4. Classification head → emotion logits (7 or 8 classes depending on dataset)

---

## Datasets & Preprocessing

**Benchmarks:**
- **RAF-DB** — 29 672 labeled images spanning seven basic emotions.  
- **FERPlus** — 35 887 grayscale images across eight emotion labels (includes *Contempt*).

**Preprocessing:**
- Resize → 224 × 224, normalize with ImageNet statistics.  
- Augmentations: random crops, flips, color jitter, Gaussian noise.

---

## Training Setup

- **Backbone:** CSWin-Tiny Transformer (pretrained)  
- **Fine-tuning:** SSF + classification head  
- **Optimizer:** Adam, learning rate = $9\times10^{-5}$, weight decay = $1\times10^{-4}$  
- **Loss:** Cross-Entropy  
- **Batch size:** 32 (16 for FERPlus due to GPU limits)  
- **Hardware:** GTX 1660 Ti (6 GB) + Google Colab T4  
- **Scheduler:** StepLR  
- **Checkpoints:** best validation model saved during training

---

## Results

<Table
  data={{
    headers: [
      { content: "Dataset", key: "dataset" },
      { content: "Accuracy (FT-CSAT)", key: "ftcsat" },
      { content: "Baseline (Yao et al., 2023)", key: "baseline" }
    ],
    rows: [
      ["RAF-DB", "84.75 %", "88.61 %"],
      ["FERPlus", "76.55 %", "89.26 %"]
    ]
  }}
/>

The results show FT-CSAT performs strongly under limited compute and demonstrates robustness to noisy, real-world images. Smaller batch sizes on FERPlus likely reduced peak performance relative to larger baselines.

---

## Observations

- The **CBAM module** improved the model’s focus on relevant facial regions, aiding generalization.  
- **SSF fine-tuning** allowed efficient adaptation with few trainable parameters.  
- Attention visualizations aligned with key facial landmarks (eyes, mouth, brow), enhancing interpretability.

---

## Limitations

1. **Compute overhead:** Transformer backbones remain heavy.  
2. **Dataset diversity:** RAF-DB and FERPlus lack broad demographic coverage.  
3. **Bias:** Requires deeper fairness evaluation across populations.

---

## Future Directions

- Train on larger, more diverse datasets.  
- Explore pruning, quantization, and distillation for real-time deployment.  
- Extend to temporal/video FER.  
- Investigate graph-based or adaptive multi-head spatial attention.  
- Combine facial + audio cues for multimodal emotion recognition.

---

## Repro (example)

```bash
python train.py \
  --model cswin_tiny \
  --ft_method ssf \
  --dataset rafdb \
  --epochs 60 \
  --lr 9e-5 \
  --batch-size 32 \
  --img-size 224
```

---

## Conclusion

FT-CSAT shows that combining transformer backbones, concise attention modules, and parameter-efficient fine-tuning yields robust FER models even with limited compute — balancing accuracy, interpretability, and efficiency.
